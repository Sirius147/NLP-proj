{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train set load & split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# dataframe을 series로 나누어 label과 data로 관리\n",
    "X_data = df['comment']\n",
    "y_data = df['toxicity']\n",
    "\n",
    "# stratify -> 레이블 비율 일정히 trainset과 validset에 분배\n",
    "# 검증을 위한 validset을 준비\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_data, y_data, test_size=.2, random_state=0, stratify=y_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 63828/63828 [00:27<00:00, 2321.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 15957/15957 [00:07<00:00, 2125.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# nttk의 tokenize함수를 통해 white tokenize 수행 후 \n",
    "# comment 별로 소문자화 후에 이중list에 저장\n",
    "def tokenize(sentences):\n",
    "  tokenized_sentences = []\n",
    "  for sent in tqdm(sentences):\n",
    "    tokenized_sent = word_tokenize(sent)\n",
    "    tokenized_sent = [word.lower() for word in tokenized_sent]\n",
    "    tokenized_sentences.append(tokenized_sent)\n",
    "  return tokenized_sentences\n",
    "\n",
    "tokenized_X_train = tokenize(X_train)\n",
    "tokenized_X_valid = tokenize(X_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어수 : 140077\n",
      "min_occur 제거 전 단어집합 크기 : 140077\n",
      "min_occur 제거 후 단어집합 크기 : 38867\n",
      "최종 단어 집합의 크기 : 38869\n"
     ]
    }
   ],
   "source": [
    "# vocab 구성을 위해 voc_list를 구성 후 counter 객체로 갯수를 맵핑\n",
    "# 중간 과제 vocab 생성 코드 참조하였음 (NGram.py)\n",
    "voc_list = []\n",
    "for sent in tokenized_X_train:\n",
    "    for word in sent:\n",
    "      voc_list.append(word)\n",
    "counters = Counter(voc_list)\n",
    "print('총 단어수 :', len(counters))\n",
    "# 이후 sorted 함수를 이용해 빈도수가 높은 단어순으로 정렬\n",
    "vocab = sorted(counters, key=counters.get, reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "min_occur = 3\n",
    "total_cnt = len(counters) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "\n",
    "for _, value in counters.items():\n",
    "    # 단어빈도가 min_occur보다 작으면\n",
    "    if(value < min_occur):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        \n",
    "\n",
    "print('min_occur 제거 전 단어집합 크기 :',total_cnt)\n",
    "# 크기로 정렬된 vocab에 index sliciing으로 min_occur voc 제거\n",
    "vocab_size = total_cnt - rare_cnt\n",
    "vocab = vocab[:vocab_size]\n",
    "print('min_occur 제거 후 단어집합 크기 :', len(vocab))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 단어집합 embedding 구성을 위해 빈도를 활용 ( encoding 된 단어는 emdedding층을 만나고 훈련과정에서 vector로 맵핑)\n",
    "# 단순 dict로 빈도수를 맵핑\n",
    "# padding과 unknown words vocab index 구성, 이를 위해 idx값을 뒤로 두칸씩\n",
    "word_to_index = {}\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<UNK>'] = 1\n",
    "\n",
    "for index, word in enumerate(vocab) :\n",
    "  word_to_index[word] = index + 2\n",
    "\n",
    "vocab_size = len(word_to_index)\n",
    "print('최종 단어 집합의 크기 :', vocab_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[573, 63, 15, 30, 68, 46, 1, 162, 3, 1385, 2, 568, 7, 91, 1054, 184, 4, 15, 87, 37, 713, 13, 268, 30, 501, 15, 96]\n",
      "[14, 71, 22, 422, 146, 5, 14, 6, 878, 1815, 643, 9, 271, 331, 1697, 143, 6, 6, 7, 59, 19, 965, 5, 3, 227, 452, 2413, 46, 14, 6, 3, 213, 75, 317, 341, 20, 2729, 96, 6, 6, 2, 26, 18, 3, 3469, 8, 3, 287, 24, 308, 2, 634, 8, 1968, 469, 5, 25622, 2, 30715, 2598, 5, 3, 36, 43, 412, 13, 62, 8, 3, 634, 8, 3, 1968, 469, 35, 3, 175, 25622, 30, 35, 71, 6866, 45, 1456, 4075, 108, 9953, 2, 20, 398, 54, 73, 1637, 16, 3, 786, 8, 3, 994, 4, 39, 7, 27, 19, 460, 20, 398, 146, 16, 69, 791, 134, 2, 34, 20, 12, 556, 5, 41, 2358, 4, 7, 481, 30715, 1731, 46, 318, 73, 292, 4, 38, 4, 34, 58, 248, 37, 157, 103, 233, 870, 4, 46, 318, 1022, 73, 8068, 138, 58, 706, 20, 474, 4, 49, 13, 15, 47, 25, 1475, 2, 78, 455, 22, 3, 53, 40, 13, 14, 6, 148, 2066, 6867, 6, 6, 22, 20, 12, 174, 3061, 8, 11, 291, 2, 34, 20, 12, 683, 78, 182, 2366, 437, 22, 78, 1794, 8, 3, 1637, 8, 994, 4, 20, 3527, 26, 294, 379, 4, 1263, 206, 12, 419, 25623, 38, 1098, 2321, 4, 227, 1, 4, 2313, 4, 39, 19, 43, 374, 4, 66, 67, 1610, 43, 1288, 667, 2, 272, 2, 434, 1312, 8, 8299, 16, 3, 1968, 1423, 5, 9953, 2, 9331, 3346, 4, 13, 3, 153, 8, 3, 287, 1823, 5, 74, 95, 39, 1598, 4, 15, 12, 79, 508, 76, 2, 41, 163, 12, 13, 20, 474, 12, 2969, 818, 4, 39, 120, 2932, 12, 174, 7329, 2, 4197, 2886, 1, 8, 85, 35, 11, 2933, 1660, 4, 39, 9953, 553, 4, 2180, 3, 2500, 3639, 8, 2988, 15461, 4, 35, 19, 30716, 45, 8299, 2, 67, 3, 474, 22, 3, 4197, 8299, 1719, 45, 3, 787, 8, 3, 15462, 622, 25, 2598, 159, 307, 36, 554, 5, 69, 2988, 475, 29, 34, 9331, 1649, 50, 282, 89, 8299, 67, 25, 16, 9953, 36, 4, 7, 86, 720, 5, 1164, 2, 362, 2, 982, 4, 9331, 198, 44, 3, 36, 3, 455, 13, 9953, 3996, 35, 1175, 1719, 45, 7330, 44, 74, 1382, 8, 3150, 66, 683, 25624, 2988, 2337, 16, 3, 614, 4, 66, 12, 1689, 4, 18, 347, 4, 45, 11, 3674, 3706, 8, 2988, 8069, 9, 2085, 16, 9953, 46, 3, 101, 2, 58, 7679, 15, 45, 3, 455, 48, 14, 6, 3, 7330, 44, 74, 1382, 8, 3, 1815, 1767, 2, 6, 6, 138, 87, 58, 111, 20, 474, 29, 3, 422, 58, 2598, 366, 5676, 284, 13, 508, 9, 7, 341, 103, 402, 2, 105, 4, 7, 4115, 93, 41, 634, 9, 59, 985, 18, 73, 470, 18, 236, 284, 108, 630, 20, 422, 170, 105, 2, 57, 4, 60, 7485, 2, 19859, 4, 2036, 1195, 4, 516, 23, 203, 21, 6]\n",
      "기존의 첫번째 샘플 : ['looks', 'like', 'it', \"'s\", 'been', 'at', '26,464', 'since', 'the', 'beginning', '.', 'unless', 'i', \"'m\", 'missing', 'something', ',', 'it', 'does', \"n't\", 'appear', 'that', 'anyone', \"'s\", 'changed', 'it', '...']\n",
      "복원된 첫번째 샘플 : ['looks', 'like', 'it', \"'s\", 'been', 'at', '<UNK>', 'since', 'the', 'beginning', '.', 'unless', 'i', \"'m\", 'missing', 'something', ',', 'it', 'does', \"n't\", 'appear', 'that', 'anyone', \"'s\", 'changed', 'it', '...']\n"
     ]
    }
   ],
   "source": [
    "def texts_to_sequences(tokenized_X_data, word_to_index):\n",
    "  encoded_X_data = []\n",
    "  for sent in tokenized_X_data:\n",
    "    index_sequences = []\n",
    "    for word in sent:\n",
    "      try:\n",
    "          index_sequences.append(word_to_index[word])\n",
    "      except KeyError:\n",
    "          index_sequences.append(word_to_index['<UNK>'])\n",
    "    encoded_X_data.append(index_sequences)\n",
    "  return encoded_X_data\n",
    "# vocab table을 활용하여 dataset의 단어를 정수로 mapping\n",
    "# 해당 함수는 로직은 쉬우나 exception 처리로 구성된 코드를 참조 링크에서 참조하였음\n",
    "encoded_X_train = texts_to_sequences(tokenized_X_train, word_to_index)\n",
    "encoded_X_valid = texts_to_sequences(tokenized_X_valid, word_to_index)\n",
    "\n",
    "for sent in encoded_X_train[:2]:\n",
    "  print(sent)\n",
    "# word_to_index의 key,value pair를 value,key pari로 저장하여 decoding에 활용할 dict 구성\n",
    "index_to_word = {}\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key\n",
    "\n",
    "decoded_sample = [index_to_word[word] for word in encoded_X_train[0]]\n",
    "print('기존의 첫번째 샘플 :', tokenized_X_train[0])\n",
    "print('복원된 첫번째 샘플 :', decoded_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padding for unequal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 4948\n",
      "리뷰의 평균 길이 : 80.12867393620354\n"
     ]
    }
   ],
   "source": [
    "# comment 별 size가 다르기에 데이터 차원을 통일 시켜야한다.\n",
    "\n",
    "print('리뷰의 최대 길이 :',max(len(review) for review in encoded_X_train))\n",
    "# data를 len() 함수로 맵핑하여 간단히 dataset의 길이의 합을 구하고 평균을 구하는 코드\n",
    "print('리뷰의 평균 길이 :',sum(map(len, encoded_X_train))/len(encoded_X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "길이가 700이하인 샘플의 비율: 99.09130788995425\n"
     ]
    }
   ],
   "source": [
    "# sample의 길이의 비율을 구하는 함수\n",
    "# comment 최대길이가 5000인데 비해\n",
    "# 평균길이는 80이어서 적절 패딩사이즈를 구하기 위함\n",
    "# 직접 max_len값을 넣어주며 적절한 padding 양을 고려하였음\n",
    "\n",
    "def len_ratio(max_len, data):\n",
    "  cnt = 0\n",
    "  for sent in data:\n",
    "    if(len(sent) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print(f\"길이가 {max_len}이하인 샘플의 비율: {(cnt/len(data))*100}\")\n",
    "\n",
    "max_len = 700\n",
    "len_ratio(max_len, encoded_X_train)\n",
    "# 700으로 하여도 전체 99%가량의 data를 cover 하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : (63828, 700)\n",
      "검증 데이터의 크기 : (15957, 700)\n"
     ]
    }
   ],
   "source": [
    "# np array로 데이터 feature 생성 및 인자값에 맞게 padding\n",
    "def pad_sequences(sentences, max_len):\n",
    "  features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "  for index, sentence in enumerate(sentences):\n",
    "    if len(sentence) != 0:\n",
    "      features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "        # sentence 길이에 해당하는 값을 feature array에 넣어준다, array 복사 연산 또한 차원을 맞춰주었음\n",
    "  return features\n",
    "\n",
    "padded_X_train = pad_sequences(encoded_X_train, max_len=max_len)\n",
    "padded_X_valid = pad_sequences(encoded_X_valid, max_len=max_len)\n",
    "\n",
    "print('훈련 데이터의 크기 :', padded_X_train.shape)\n",
    "print('검증 데이터의 크기 :', padded_X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"사용 device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# label data를 우선 np array로 만들고 tensor로 변환\n",
    "train_label_tensor = torch.tensor(np.array(y_train))\n",
    "valid_label_tensor = torch.tensor(np.array(y_valid))\n",
    "print(train_label_tensor[:30])\n",
    "\n",
    "\n",
    "# 최종 단어 집합크기, em_dim, hid_dim, out_dim (label 2개에 맞추어 2로 설정) , dropout_p -> 50% 제공\n",
    "# Embedding 층을 통해 단어집합을 vector로구성\n",
    "# lstm 모델을 사용, 0차원을 batch로구성\n",
    "# 최종 output 출력을 위한 linear layer -> 2\n",
    "class TextModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout_p = 0.5):\n",
    "        super(TextModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch_size, seqence 길이) -> (batch_size, seqence 길이, embedding_dim)\n",
    "\n",
    "        # LSTM은 (hidden state, cell state)를 반환한다\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)  # lstm_out: (batch_size, seqence 길이, hidden_dim), hidden: (1, batch_size, hidden_dim)\n",
    "\n",
    "        last_hidden = hidden.squeeze(0)  # (batch_size, hidden_dim)\n",
    "        logits = self.fc(last_hidden)  # (batch_size, output_dim) # 최종 2개의 label값에 대한 추론\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 배치의 수 : 1995\n"
     ]
    }
   ],
   "source": [
    "encoded_train = torch.tensor(padded_X_train).to(torch.int64)\n",
    "train_dataset = torch.utils.data.TensorDataset(encoded_train, train_label_tensor)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
    "# train set 및 valid set또한 학습을 위해 tensor로 변환, 이후\n",
    "# dataset module을 통해 학습 데이터 구성\n",
    "# dataloader를 통해 batch_size 32으로 학습진행 구성\n",
    "# valid set의 경우 batch_size 1로\n",
    "encoded_valid = torch.tensor(padded_X_valid).to(torch.int64)\n",
    "valid_dataset = torch.utils.data.TensorDataset(encoded_valid, valid_label_tensor)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, shuffle=True, batch_size=1)\n",
    "\n",
    "total_batch = len(train_dataloader)\n",
    "print('총 배치의 수 : {}'.format(total_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper param 설정\n",
    "# epoch의 경우 실험을 통해 accuracy가 준수한 수치를 채택하였음\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "model = TextModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "model.to(device)\n",
    "# loss function으로 crossentropy, optimizer는 Adam으로 구성\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(logits, labels):\n",
    "    # argmax -> dim =1 이면 행기준 max값의 index 출력\n",
    "    # 해당값이 label과 일치할 경우의 합을 더하여 accuracy return\n",
    "    predicted = torch.argmax(logits, dim=1)\n",
    "    correct = (predicted == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_dataloader, criterion, device):\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    model.eval()\n",
    "    # 가중치 갱신 없이 (검증)\n",
    "    with torch.no_grad():\n",
    "        # valid set 에서  배치(1)만큼의 데이터 load\n",
    "        for batch_X, batch_y in valid_dataloader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            # 모델의 예측값\n",
    "            logits = model(batch_X)\n",
    "\n",
    "            # 손실을 계산\n",
    "            loss = criterion(logits, batch_y)\n",
    "\n",
    "            # 정확도와 손실을 계산함\n",
    "            val_loss += loss.item()\n",
    "            val_correct += calculate_accuracy(logits, batch_y) * batch_y.size(0)\n",
    "            val_total += batch_y.size(0)\n",
    "\n",
    "    val_accuracy = val_correct / val_total\n",
    "    val_loss /= len(valid_dataloader)\n",
    "    # ross 와 accuracy return\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [01:11<10:39, 71.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 0.3168, Train Accuracy: 0.9044\n",
      "Validation Loss: 0.3152, Validation Accuracy: 0.9046\n",
      "Validation loss improved from inf to 0.3152. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|████████████████▌                                                                  | 2/10 [02:19<09:13, 69.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "Train Loss: 0.3148, Train Accuracy: 0.9048\n",
      "Validation Loss: 0.3149, Validation Accuracy: 0.9046\n",
      "Validation loss improved from 0.3152 to 0.3149. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30%|████████████████████████▉                                                          | 3/10 [03:28<08:06, 69.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "Train Loss: 0.3140, Train Accuracy: 0.9049\n",
      "Validation Loss: 0.3160, Validation Accuracy: 0.9045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [04:40<07:02, 70.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "Train Loss: 0.3134, Train Accuracy: 0.9051\n",
      "Validation Loss: 0.3188, Validation Accuracy: 0.9040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [05:53<05:56, 71.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "Train Loss: 0.3130, Train Accuracy: 0.9052\n",
      "Validation Loss: 0.3180, Validation Accuracy: 0.9043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [07:06<04:46, 71.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "Train Loss: 0.3133, Train Accuracy: 0.9052\n",
      "Validation Loss: 0.3173, Validation Accuracy: 0.9041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [08:17<03:34, 71.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "Train Loss: 0.2295, Train Accuracy: 0.9223\n",
      "Validation Loss: 0.1610, Validation Accuracy: 0.9452\n",
      "Validation loss improved from 0.3149 to 0.1610. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [09:28<02:23, 71.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "Train Loss: 0.1458, Train Accuracy: 0.9437\n",
      "Validation Loss: 0.1419, Validation Accuracy: 0.9495\n",
      "Validation loss improved from 0.1610 to 0.1419. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [10:39<01:11, 71.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "Train Loss: 0.1025, Train Accuracy: 0.9633\n",
      "Validation Loss: 0.1300, Validation Accuracy: 0.9552\n",
      "Validation loss improved from 0.1419 to 0.1300. 체크포인트를 저장합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [11:51<00:00, 71.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "Train Loss: 0.0789, Train Accuracy: 0.9719\n",
      "Validation Loss: 0.1344, Validation Accuracy: 0.9573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# 검증 loss init\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "# tqdm으로 진행상태 표시\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    model.train()\n",
    "    for batch_X, batch_y in train_dataloader:\n",
    "        # Forward pass (32 size batch)\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        # batch_X.shape == (batch_size, max_len)\n",
    "        logits = model(batch_X)\n",
    "\n",
    "        # loss 계산\n",
    "        loss = criterion(logits, batch_y)\n",
    "        # 가중치 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 오류 역전파\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate training accuracy and loss\n",
    "        train_loss += loss.item()\n",
    "        train_correct += calculate_accuracy(logits, batch_y) * batch_y.size(0)\n",
    "        train_total += batch_y.size(0)\n",
    "\n",
    "    train_accuracy = train_correct / train_total\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, device)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # 검증 손실이 최소일 때 체크포인트 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. 체크포인트를 저장합니다.')\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation loss: 0.1300\n",
      "Best validation accuracy: 0.9552\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "model.load_state_dict(torch.load('best_model_checkpoint.pth'))\n",
    "\n",
    "# 모델을 device에 올립니다.\n",
    "model.to(device)\n",
    "# 검증 데이터에 대한 정확도와 손실 계산\n",
    "val_loss, val_accuracy = evaluate(model, valid_dataloader, criterion, device)\n",
    "\n",
    "print(f'Best validation loss: {val_loss:.4f}')\n",
    "print(f'Best validation accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_tag = {0 : 'normal', 1 : 'toxicity'}\n",
    "\n",
    "def predict(text, model, word_to_index, index_to_tag):\n",
    "    # 모델 평가 모드\n",
    "    model.eval()\n",
    "\n",
    "    # test sentence를 단순 white tokenize\n",
    "    # unknown word는 1 할당\n",
    "    tokens = word_tokenize(text)\n",
    "    token_indices = [word_to_index.get(token.lower(), 1) for token in tokens]\n",
    "\n",
    "    # 리스트를 텐서로 변환\n",
    "    input_tensor = torch.tensor([token_indices], dtype=torch.long).to(device)  # (1, seq_length)\n",
    "\n",
    "    # 당연히 가중치 갱신없이 예측\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)  # (1, output_dim)\n",
    "        \n",
    "    probs = F.softmax(logits, dim=1) # softmax함수를 통해 확률값으로 mapping 및 최대확률값 도출\n",
    "    # 레이블 인덱스 예측, max함수는 최댓값과 idx를 동시에 return\n",
    "    prob, predicted_index = torch.max(probs, dim=1)  \n",
    "\n",
    "    # 인덱스와 매칭되는 카테고리 문자열로 변환\n",
    "    prob_value = prob.item()\n",
    "    predicted_tag = index_to_tag[predicted_index.item()]\n",
    "\n",
    "    return prob_value, predicted_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9992402791976929 toxicity\n"
     ]
    }
   ],
   "source": [
    "# 가벼운 예시로 test 결과 출력\n",
    "test_input = \"I hate this shit\"\n",
    "prob_value, predicted_tag = predict(test_input, model, word_to_index, index_to_tag)\n",
    "print(prob_value, predicted_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 31915/31915 [00:47<00:00, 669.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# test file load\n",
    "\n",
    "df_test = pd.read_csv(\"test_for_inference.csv\")\n",
    "X_test = df_test[\"comment\"]\n",
    "# 예측값을 제출형식에 맞추기 위한 list (probability와 pred 열)\n",
    "prob_values = []\n",
    "predicted_tags = []\n",
    "for txt in tqdm(X_test):\n",
    "    prob_value, predicted_tag = predict(txt, model, word_to_index, index_to_tag)\n",
    "    prob_values.append(prob_value)\n",
    "    predicted_tags.append(predicted_tag)\n",
    "# 해당 list로 data frame 구성후 csv파일로 저장\n",
    "result_df = pd.DataFrame({\n",
    "    'probability': prob_values,\n",
    "    'pred': predicted_tags\n",
    "})\n",
    "result_df.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
